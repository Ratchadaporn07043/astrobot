import os
import io
import base64
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
from dotenv import load_dotenv
from pymongo import MongoClient
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
import torch
import easyocr
from openai import OpenAI
from datetime import datetime
import json
import gc
import psutil
import re

# üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° PyThaiNLP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á OCR
try:
    from pythainlp import word_tokenize
    from pythainlp.spell import correct
    from pythainlp.util import normalize
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ PyThaiNLP loaded successfully")
except ImportError:
    PYTHAINLP_AVAILABLE = False
    print("‚ö†Ô∏è PyThaiNLP not available, using basic text processing")

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ MPS device, PIL.ANTIALIAS ‡πÅ‡∏•‡∏∞ tokenizers parallelism
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î .env
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".env"))
load_dotenv(dotenv_path)

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö
PDF_PATH = "data/attention.pdf"
MONGO_URL = os.getenv("MONGO_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUMMARY_DB_NAME = "astrobot_summary"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà summary ‡πÅ‡∏•‡∏∞ summary embedding ‡πÅ‡∏•‡πâ‡∏ß
ORIGINAL_DB_NAME = "astrobot_original"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà extract ‡πÅ‡∏•‡πâ‡∏ß

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö - Collection Names
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (ORIGINAL_DB_NAME)
ORIGINAL_TEXT_COLLECTION = "original_text_chunks"
ORIGINAL_IMAGE_COLLECTION = "original_image_chunks"
ORIGINAL_TABLE_COLLECTION = "original_table_chunks"

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (SUMMARY_DB_NAME)
PROCESSED_TEXT_COLLECTION = "processed_text_chunks"
PROCESSED_IMAGE_COLLECTION = "processed_image_chunks"
PROCESSED_TABLE_COLLECTION = "processed_table_chunks"

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á bbox ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà MongoDB ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode ‡πÑ‡∏î‡πâ
def convert_bbox_to_mongodb_format(bbox):
    """
    ‡πÅ‡∏õ‡∏•‡∏á bbox (pymupdf.Rect, tuple, ‡∏´‡∏£‡∏∑‡∏≠ None) ‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà MongoDB ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode ‡πÑ‡∏î‡πâ
    
    Args:
        bbox: pymupdf.Rect, tuple (x0, y0, x1, y1), ‡∏´‡∏£‡∏∑‡∏≠ None
        
    Returns:
        tuple ‡∏´‡∏£‡∏∑‡∏≠ None: (x0, y0, x1, y1) ‡∏´‡∏£‡∏∑‡∏≠ None
    """
    if bbox is None:
        return None
    
    try:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô pymupdf.Rect object
        if hasattr(bbox, 'x0') and hasattr(bbox, 'y0') and hasattr(bbox, 'x1') and hasattr(bbox, 'y1'):
            return (float(bbox.x0), float(bbox.y0), float(bbox.x1), float(bbox.y1))
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏´‡∏£‡∏∑‡∏≠ list
        elif isinstance(bbox, (tuple, list)) and len(bbox) >= 4:
            return (float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3]))
        else:
            return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error converting bbox: {e}")
        return None

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory
def check_memory():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory"""
    memory = psutil.virtual_memory()
    print(f"üíæ Memory: {memory.percent}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
    if memory.percent > 80:
        print("‚ö†Ô∏è High memory usage, running garbage collection...")
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

# üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
def improve_thai_ocr_text(ocr_text):
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
    """
    if not PYTHAINLP_AVAILABLE or not ocr_text.strip():
        return ocr_text
    
    try:
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text = ocr_text.strip()
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î
        text = re.sub(r'([‡∏Å-‡πô])([A-Za-z])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
        text = re.sub(r'([A-Za-z])([‡∏Å-‡πô])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©-‡πÑ‡∏ó‡∏¢
        text = re.sub(r'([‡∏Å-‡πô])([0-9])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        text = re.sub(r'([0-9])([‡∏Å-‡πô])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç-‡πÑ‡∏ó‡∏¢
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
        text = re.sub(r'\s+', ' ', text)
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
        words = word_tokenize(text, engine='newmm')
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
        corrected_words = []
        for word in words:
            if len(word) > 2 and word.isalpha():  # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                try:
                    corrected = correct(word)
                    corrected_words.append(corrected if corrected else word)
                except:
                    corrected_words.append(word)
            else:
                corrected_words.append(word)
        
        # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        improved_text = ' '.join(corrected_words)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        improved_text = re.sub(r'\s+', ' ', improved_text).strip()
        
        return improved_text
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error in Thai text improvement: {e}")
        return ocr_text

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö lazy loading
def get_embedding_model():
    """‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_embedding_model, 'model'):
        print("üîÑ Loading embedding model...")
        get_embedding_model.model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
    return get_embedding_model.model

def get_semantic_model():
    """‡πÇ‡∏´‡∏•‡∏î semantic model ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_semantic_model, 'model'):
        print("üîÑ Loading semantic model...")
        get_semantic_model.model = SentenceTransformer("minishlab/potion-multilingual-128M", device="cpu")
    return get_semantic_model.model

def get_ocr_reader():
    """‡πÇ‡∏´‡∏•‡∏î OCR reader ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_ocr_reader, 'reader'):
        print(" Loading OCR reader...")
        get_ocr_reader.reader = easyocr.Reader(['en', 'th'], gpu=False, verbose=False)
    return get_ocr_reader.reader

# üÜï ‡πÇ‡∏´‡∏•‡∏î Image Embedding Model (CLIP) ‡πÅ‡∏ö‡∏ö lazy loading
def get_image_embedding_model():
    """‡πÇ‡∏´‡∏•‡∏î CLIP model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á image embeddings ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_image_embedding_model, 'model'):
        try:
            print("üîÑ Loading CLIP image embedding model...")
            # ‡πÉ‡∏ä‡πâ CLIP model ‡∏à‡∏≤‡∏Å sentence-transformers
            get_image_embedding_model.model = SentenceTransformer('clip-ViT-B-32', device="cpu")
            print("‚úÖ CLIP model loaded successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load CLIP model: {e}")
            print("‚ö†Ô∏è Image embeddings will be disabled")
            get_image_embedding_model.model = None
    return get_image_embedding_model.model

# OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
def extract_text_with_pymupdf(path):
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
    """
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    text_output = ""
    doc = fitz.open(path)
    
    try:
        for page_num, page in enumerate(doc):
            page_text = page.get_text("text")
            if page_text.strip():
                text_output += f"\n--- ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ---\n{page_text}"
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 20 ‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 20 == 0:
                check_memory()
                
    finally:
        doc.close()
    
    return text_output

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory management)
def extract_images_with_ocr(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP
    """
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    images_data = []
    doc = fitz.open(path)
    
    try:
        ocr_reader = get_ocr_reader()
        
        for page_num, page in enumerate(doc):
            images = page.get_images(full=True)
            print(f"‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}: {len(images)} ‡∏£‡∏π‡∏õ")
            
            for img_index, img in enumerate(images):
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    image = Image.open(io.BytesIO(image_bytes))
                    width, height = image.size
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width * height > 1500000:  # 1.5M pixels
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà {img_index + 1} ({width}x{height})")
                        continue
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width < 50 or height < 50:
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å {img_index + 1} ({width}x{height})")
                        continue
                    
                    # OCR
                    ocr_results = ocr_reader.readtext(image_bytes)
                    ocr_text = " ".join([result[1] for result in ocr_results if result[2] > 0.3])  # ‡∏•‡∏î confidence threshold
                    
                    if ocr_text.strip():
                        # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                        improved_text = improve_thai_ocr_text(ocr_text)
                        
                        image_info = {
                            "page": page_num + 1,
                            "image_index": img_index + 1,
                            "original_text": ocr_text.strip(),
                            "improved_text": improved_text,
                            "text": improved_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                            "image_base64": base64.b64encode(image_bytes).decode("utf-8")
                        }
                        images_data.append(image_info)
                        
                        print(f"‚úÖ ‡∏£‡∏π‡∏õ {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                    
                    # ‡∏•‡πâ‡∏≤‡∏á memory
                    del image, image_bytes, ocr_results
                    
                except Exception as e:
                    print(f"‚ùó Error processing image {img_index + 1} on page {page_num + 1}: {e}")
                    continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 5 == 0:
                check_memory()
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
            if len(images_data) > 50:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 50 ‡∏£‡∏π‡∏õ
                print("‚ö†Ô∏è ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 50 ‡∏£‡∏π‡∏õ")
                break
                
    finally:
        doc.close()
    
    return images_data

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber
def extract_tables_with_pdfplumber(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber
    """
    print(f" ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    tables_data = []
    
    try:
        with pdfplumber.open(path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                for table_index, table in enumerate(tables):
                    if table:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                        table_text = ""
                        for row in table:
                            if row:
                                row_text = " | ".join([cell if cell else "" for cell in row])
                                table_text += row_text + "\n"
                        
                        if table_text.strip():
                            table_info = {
                                "page": page_num + 1,
                                "table_index": table_index + 1,
                                "text": table_text.strip()
                            }
                            tables_data.append(table_info)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 10 ‡∏´‡∏ô‡πâ‡∏≤
                if page_num % 10 == 0:
                    check_memory()
                    
    except Exception as e:
        print(f"‚ùó Error extracting tables: {e}")
    
    return tables_data

# ‚úÖ Semantic Chunking ‡∏î‡πâ‡∏ß‡∏¢ Potion Model
def semantic_chunking_with_potion(text, content_type, chunk_size=1000, overlap=200):
    """
    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ Semantic Chunking ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Potion Model
    """
    print(f"üß† ‡πÄ‡∏£‡∏¥‡πà‡∏° Semantic Chunking ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {content_type.upper()}")
    
    try:
        semantic_model = get_semantic_model()
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        sentences = text.split('. ')
        if len(sentences) <= 1:
            return [{"text": text, "type": content_type, "chunk_id": 0}]
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
        if len(sentences) > 500:
            sentences = sentences[:500]
            print(f"‚ö†Ô∏è ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà 500 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        sentence_embeddings = semantic_model.encode(sentences)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        chunks = []
        current_chunk = []
        current_length = 0
        
        for i, sentence in enumerate(sentences):
            current_chunk.append(sentence)
            current_length += len(sentence)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ö‡πà‡∏á chunk ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if current_length >= chunk_size or i == len(sentences) - 1:
                chunk_text = '. '.join(current_chunk)
                chunks.append({
                    "text": chunk_text,
                    "type": content_type,
                    "chunk_id": len(chunks)
                })
                current_chunk = []
                current_length = 0
        
        # ‡∏•‡πâ‡∏≤‡∏á memory
        del sentence_embeddings, sentences
        check_memory()
        
        return chunks
        
    except Exception as e:
        print(f"‚ùó Error in semantic chunking: {e}")
        # Fallback: ‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
        return [{"text": text, "type": content_type, "chunk_id": 0}]

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Embeddings
def create_embeddings(text):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    """
    try:
        embedding_model = get_embedding_model()
        return embedding_model.encode(text).tolist()
    except Exception as e:
        print(f"‚ùó Error creating embeddings: {e}")
        return [0.0] * 384  # fallback vector

# üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á Image Embeddings
def create_image_embeddings(image_bytes):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ CLIP model
    
    Args:
        image_bytes: bytes ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        
    Returns:
        list: image embedding vector ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ
    """
    try:
        image_model = get_image_embedding_model()
        if image_model is None:
            print("   ‚ö†Ô∏è Image embedding model not available, skipping...")
            return None
        
        # ‡πÅ‡∏õ‡∏•‡∏á image bytes ‡πÄ‡∏õ‡πá‡∏ô PIL Image
        image = Image.open(io.BytesIO(image_bytes))
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏î‡πâ‡∏ß‡∏¢ CLIP
        embedding = image_model.encode(image)
        return embedding.tolist()
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error creating image embeddings: {e}")
        return None

# ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OpenAI
def summarize_with_openai(text, content_type, timeout=30, max_retries=3):
    """
    ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OpenAI GPT
    
    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ
        content_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (text/image/table)
        timeout: ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        max_retries: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error
    """
    for attempt in range(max_retries):
        try:
            prompt = f"""
            ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢):
            
            ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {content_type}
            ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {text[:2000]}...
            
            ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
            """
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡πÅ‡∏•‡∏∞ error handling
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_completion_tokens=150,
                temperature=0.7,
                timeout=timeout  # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            error_msg = str(e)
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2  # Exponential backoff: 2s, 4s, 6s
                print(f"   ‚ö†Ô∏è Error in summarization (attempt {attempt + 1}/{max_retries}): {error_msg[:100]}")
                print(f"   ‚è≥ ‡∏£‡∏≠ {wait_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà...")
                import time
                time.sleep(wait_time)
            else:
                print(f"   ‚ùó Error in summarization after {max_retries} attempts: {error_msg[:100]}")
                # Fallback: ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
                return text[:200] + "..." if len(text) > 200 else text
    
    # Fallback ‡∏ñ‡πâ‡∏≤ retry ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
    return text[:200] + "..." if len(text) > 200 else text

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á MongoDB (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
def store_original_data_in_mongodb(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á ORIGINAL_DB_NAME (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡πÉ‡∏ä‡πâ ORIGINAL_DB_NAME ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        db_name = ORIGINAL_DB_NAME
        print(f"üìä ‡πÉ‡∏ä‡πâ Database: {db_name} (Original - ‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings/summary)")
        
        db = client[db_name]
        collection = db[collection_name]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        collection.delete_many({})
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now()
            
            # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° embeddings ‡πÅ‡∏•‡∏∞ summary
            collection.insert_one(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(chunks)} chunks ‡∏•‡∏á {collection_name}")
        client.close()
        
    except Exception as e:
        print(f"‚ùó MongoDB Atlas connection failed: {e}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏ó‡∏ô...")
        
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        store_original_to_json(chunks, collection_name)

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á MongoDB (‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
def store_processed_data_in_mongodb(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á SUMMARY_DB_NAME (‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡πÉ‡∏ä‡πâ SUMMARY_DB_NAME ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
        db_name = SUMMARY_DB_NAME
        print(f"üìä ‡πÉ‡∏ä‡πâ Database: {db_name} (Processed - ‡∏°‡∏µ summary embeddings/summary)")
        
        db = client[db_name]
        collection = db[collection_name]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        collection.delete_many({})
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (‡∏°‡∏µ summary embeddings ‡πÅ‡∏•‡∏∞ summary)
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
            processed_chunk = chunk.copy()
            processed_chunk["created_at"] = datetime.now()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏Å‡πà‡∏≠‡∏ô
            summary_text = summarize_with_openai(chunk["text"], chunk["type"])
            processed_chunk["summary"] = summary_text
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏à‡∏≤‡∏Å summary ‡πÅ‡∏ó‡∏ô text ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            processed_chunk["embeddings"] = create_embeddings(summary_text)
            
            collection.insert_one(processed_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 3 chunks
            if i % 3 == 0:
                check_memory()
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß {len(chunks)} chunks ‡∏•‡∏á {collection_name}")
        client.close()
        
    except Exception as e:
        print(f"‚ùó MongoDB Atlas connection failed: {e}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏ó‡∏ô...")
        
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        store_processed_to_json(chunks, collection_name)

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (fallback)
def store_original_to_json(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô fallback (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
        original_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now().isoformat()
            
            # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° embeddings ‡πÅ‡∏•‡∏∞ summary
            original_chunks.append(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        filename = f"{output_dir}/{collection_name}_original.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(original_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(original_chunks)} chunks ‡∏•‡∏á {filename}")
        
    except Exception as e:
        print(f"‚ùó Error saving original data to JSON: {e}")

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (fallback)
def store_processed_to_json(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô fallback (‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunks
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
            processed_chunk = chunk.copy()
            processed_chunk["created_at"] = datetime.now().isoformat()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏Å‡πà‡∏≠‡∏ô
            summary_text = summarize_with_openai(chunk["text"], chunk["type"])
            processed_chunk["summary"] = summary_text
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏à‡∏≤‡∏Å summary ‡πÅ‡∏ó‡∏ô text ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            processed_chunk["embeddings"] = create_embeddings(summary_text)
            processed_chunks.append(processed_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 3 chunks
            if i % 3 == 0:
                check_memory()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        filename = f"{output_dir}/{collection_name}_processed.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(processed_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß {len(processed_chunks)} chunks ‡∏•‡∏á {filename}")
        
    except Exception as e:
        print(f"‚ùó Error saving processed data to JSON: {e}")

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏ï‡∏≤‡∏° flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö - ‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô)
def process_single_page(page_num, pymupdf_page, pdfplumber_pdf, ocr_reader, doc_id_counter):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: Extract ‚Üí Summary ‚Üí Embedding ‚Üí Store
    üÜï ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô) - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° y-coordinate
    
    Args:
        page_num: ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (0-based)
        pymupdf_page: ‡∏´‡∏ô‡πâ‡∏≤ PDF ‡∏à‡∏≤‡∏Å PyMuPDF
        pdfplumber_pdf: PDF object ‡∏à‡∏≤‡∏Å pdfplumber
        ocr_reader: OCR reader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        doc_id_counter: counter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        
    Returns:
        dict: {
            'has_content': bool,  # ‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤)
            'text_chunks': list,
            'image_chunks': list,
            'table_chunks': list,
            'text_processed_chunks': list,
            'image_processed_chunks': list,
            'table_processed_chunks': list
        }
    """
    page_results = {
        'has_content': False,
        'text_chunks': [],
        'image_chunks': [],
        'table_chunks': [],
        'text_processed_chunks': [],
        'image_processed_chunks': [],
        'table_processed_chunks': []
    }
    
    try:
        print(f"\n{'='*50}")
        print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} (‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠)")
        print(f"{'='*50}")
        
        # === STEP 1: ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° elements ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ===
        elements = []  # ‡πÄ‡∏Å‡πá‡∏ö elements ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á y-coordinate
        
        # 1.1 ‡∏î‡∏∂‡∏á Text Blocks ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        text_blocks = pymupdf_page.get_text("blocks")  # Returns: [(x0, y0, x1, y1, text, block_no, block_type), ...]
        for block in text_blocks:
            if block[6] == 0:  # block_type = 0 ‡∏Ñ‡∏∑‡∏≠ text block
                x0, y0, x1, y1, text, block_no, block_type = block
                if text.strip():
                    elements.append({
                        'type': 'text',
                        'y_pos': y0,  # ‡πÉ‡∏ä‡πâ y0 (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ö‡∏ô‡∏™‡∏∏‡∏î) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                        'data': {
                            'text': text.strip(),
                            'bbox': (x0, y0, x1, y1),
                            'block_no': block_no
                        }
                    })
        
        # 1.2 ‡∏î‡∏∂‡∏á Images ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        images = pymupdf_page.get_images(full=True)
        if images:
            print(f"   üñºÔ∏è ‡∏û‡∏ö {len(images)} ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ")
        
        for img_index, img in enumerate(images):
            xref = img[0]
            try:
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ bbox ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å get_image_rects
                y_pos = 0  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                bbox = None
                try:
                    from pymupdf.utils import get_image_rects
                    image_rects = get_image_rects(pymupdf_page, xref)
                    if image_rects:
                        bbox = image_rects[0]  # ‡πÉ‡∏ä‡πâ rect ‡πÅ‡∏£‡∏Å
                        if hasattr(bbox, 'y0'):
                            y_pos = bbox.y0
                        elif isinstance(bbox, (list, tuple)) and len(bbox) >= 4:
                            y_pos = bbox[1]  # y0
                except Exception as rect_error:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å image list position
                    # (‡∏£‡∏π‡∏õ‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ö‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏ß‡πà‡∏≤)
                    y_pos = img_index * 100  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                
                elements.append({
                    'type': 'image',
                    'y_pos': y_pos,
                    'data': {
                        'xref': xref,
                        'image_index': img_index,
                        'bbox': bbox
                    }
                })
            except Exception as e:
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏£‡∏π‡∏õ {img_index + 1} ‡πÑ‡∏î‡πâ: {e}")
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á 0 (‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î)
                elements.append({
                    'type': 'image',
                    'y_pos': img_index * 100,  # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                    'data': {
                        'xref': xref,
                        'image_index': img_index,
                        'bbox': None
                    }
                })
        
        # 1.3 ‡∏î‡∏∂‡∏á Tables ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏à‡∏≤‡∏Å pdfplumber)
        if page_num < len(pdfplumber_pdf.pages):
            pdfplumber_page = pdfplumber_pdf.pages[page_num]
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ bbox ‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            try:
                # ‡πÉ‡∏ä‡πâ find_tables ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ bbox (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                if hasattr(pdfplumber_page, 'find_tables'):
                    table_objects = pdfplumber_page.find_tables()
                    
                    for table_index, table_obj in enumerate(table_objects):
                        if table_obj and hasattr(table_obj, 'bbox'):
                            bbox = table_obj.bbox
                            y_pos = bbox[1] if isinstance(bbox, (list, tuple)) else getattr(bbox, 'y0', bbox[1])
                            
                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                            table = table_obj.extract() if hasattr(table_obj, 'extract') else None
                            table_text = ""
                            if table:
                                for row in table:
                                    if row:
                                        row_text = " | ".join([cell if cell else "" for cell in row])
                                        table_text += row_text + "\n"
                            
                            if table_text.strip():
                                elements.append({
                                    'type': 'table',
                                    'y_pos': y_pos,
                                    'data': {
                                        'table_index': table_index,
                                        'text': table_text.strip(),
                                        'bbox': bbox
                                    }
                                })
                else:
                    raise AttributeError("find_tables not available")
            except Exception as e:
                # Fallback: ‡∏ñ‡πâ‡∏≤ find_tables ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ extract_tables ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ find_tables ‡πÑ‡∏î‡πâ: {e}, ‡πÉ‡∏ä‡πâ extract_tables ‡πÅ‡∏ó‡∏ô")
                tables = pdfplumber_page.extract_tables()
                
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á text ‡πÅ‡∏•‡∏∞ image elements ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                existing_y_positions = [e['y_pos'] for e in elements]
                base_y_pos = max(existing_y_positions) if existing_y_positions else 500  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà 500 ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ elements ‡∏≠‡∏∑‡πà‡∏ô
                
                for table_index, table in enumerate(tables):
                    if table:
                        table_text = ""
                        for row in table:
                            if row:
                                row_text = " | ".join([cell if cell else "" for cell in row])
                                table_text += row_text + "\n"
                        
                        if table_text.strip():
                            # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å elements ‡∏≠‡∏∑‡πà‡∏ô‡πÜ)
                            table_y_pos = base_y_pos + (table_index * 150)
                            elements.append({
                                'type': 'table',
                                'y_pos': table_y_pos,
                                'data': {
                                    'table_index': table_index,
                                    'text': table_text.strip(),
                                    'bbox': None
                                }
                            })
        
        # === STEP 2: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö elements ‡∏ï‡∏≤‡∏° y-coordinate (‡∏à‡∏≤‡∏Å‡∏ö‡∏ô‡∏•‡∏á‡∏•‡πà‡∏≤‡∏á) ===
        elements.sort(key=lambda x: x['y_pos'])
        
        print(f"üìä ‡∏û‡∏ö {len(elements)} elements: {len([e for e in elements if e['type']=='text'])} text, "
              f"{len([e for e in elements if e['type']=='image'])} images, "
              f"{len([e for e in elements if e['type']=='table'])} tables")
        
        # === STEP 3: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô) ===
        text_chunk_counter = 0
        image_chunk_counter = 0
        table_chunk_counter = 0
        
        for element_index, element in enumerate(elements):
            element_type = element['type']
            data = element['data']
            
            print(f"\nüìå Element {element_index + 1}/{len(elements)}: {element_type.upper()} "
                  f"(y={element['y_pos']:.1f})")
            
            if element_type == 'text':
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Text Block
                page_results['has_content'] = True
                text_content = data['text']
                print(f"   üìù Text: {len(text_content)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                
                text_chunk = {
                    "text": text_content,
                    "type": "text",
                    "chunk_id": text_chunk_counter,
                    "page": page_num + 1,
                    "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_text_{text_chunk_counter}",
                    "bbox": convert_bbox_to_mongodb_format(data['bbox'])
                }
                # ‚úÖ Original chunk: ‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings (‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
                page_results['text_chunks'].append(text_chunk)
                text_chunk_counter += 1
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö processed chunk
                print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á summary...")
                summary_text = summarize_with_openai(text_chunk["text"], "text")
                print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings...")
                text_processed_chunk = text_chunk.copy()
                text_processed_chunk["summary"] = summary_text
                text_processed_chunk["embeddings"] = create_embeddings(summary_text)  # embeddings ‡∏à‡∏≤‡∏Å summary
                text_processed_chunk["created_at"] = datetime.now()
                page_results['text_processed_chunks'].append(text_processed_chunk)
                print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡πÅ‡∏•‡πâ‡∏ß")
            
            elif element_type == 'image':
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Image
                xref = data['xref']
                img_index = data['image_index']
                
                try:
                    print(f"   üñºÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_index + 1}...")
                    base_image = pymupdf_page.parent.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    image = Image.open(io.BytesIO(image_bytes))
                    width, height = image.size
                    print(f"   üìè ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {width}x{height} pixels")
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width * height > 1500000:
                        print(f"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà ({width}x{height}, {width*height:,} pixels > 1,500,000)")
                        del image, image_bytes
                        continue
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width < 50 or height < 50:
                        print(f"   ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å ({width}x{height} < 50x50)")
                        del image, image_bytes
                        continue
                    
                    # OCR
                    print(f"   üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥ OCR...")
                    ocr_results = ocr_reader.readtext(image_bytes)
                    ocr_text = " ".join([result[1] for result in ocr_results if result[2] > 0.3])
                    
                    if ocr_text.strip():
                        page_results['has_content'] = True
                        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                        improved_text = improve_thai_ocr_text(ocr_text)
                        
                        print(f"   üñºÔ∏è Image {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (OCR: {len(ocr_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)")
                        
                        # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding (‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö image_bytes)
                        print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding...")
                        image_embedding = create_image_embeddings(image_bytes)
                        
                        # Create image chunk
                        image_chunk = {
                            "text": improved_text,
                            "type": "image",
                            "chunk_id": image_chunk_counter,
                            "page": page_num + 1,
                            "image_index": img_index + 1,
                            "original_text": ocr_text.strip(),
                            "improved_text": improved_text,
                            "image_base64": base64.b64encode(image_bytes).decode("utf-8"),
                            "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_img_{img_index + 1}",
                            "bbox": convert_bbox_to_mongodb_format(data['bbox'])
                        }
                        # ‚úÖ Original chunk: ‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings (‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
                        page_results['image_chunks'].append(image_chunk)
                        image_chunk_counter += 1
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö processed chunk
                        print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á summary...")
                        summary_text = summarize_with_openai(image_chunk["text"], "image")
                        print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings...")
                        text_embedding = create_embeddings(summary_text)
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö processed chunk
                        if image_embedding is not None:
                            print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(image_embedding)} dimensions)")
                        
                        image_processed_chunk = image_chunk.copy()
                        image_processed_chunk["summary"] = summary_text
                        image_processed_chunk["embeddings"] = text_embedding  # text embedding ‡∏à‡∏≤‡∏Å summary
                        image_processed_chunk["created_at"] = datetime.now()
                        
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏° image embedding ‡πÉ‡∏ô processed chunk ‡∏î‡πâ‡∏ß‡∏¢
                        if image_embedding is not None:
                            image_processed_chunk["image_embeddings"] = image_embedding
                        
                        page_results['image_processed_chunks'].append(image_processed_chunk)
                        print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á summary, text embeddings ‡πÅ‡∏•‡∏∞ image embeddings ‡πÅ‡∏•‡πâ‡∏ß")
                    else:
                        print(f"   ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_index + 1} (OCR ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°) - ‡∏Ç‡πâ‡∏≤‡∏°")
                    
                    # ‡∏•‡πâ‡∏≤‡∏á memory
                    del image, image_bytes, ocr_results
                    
                except Exception as e:
                    print(f"   ‚ùó Error processing image {img_index + 1}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            elif element_type == 'table':
                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Table
                table_text = data['text']
                table_index = data['table_index']
                
                if table_text.strip():
                    page_results['has_content'] = True
                    print(f"   üìä Table {table_index + 1}: {len(table_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                    
                    # Create table chunk
                    table_chunk = {
                        "text": table_text,
                        "type": "table",
                        "chunk_id": table_chunk_counter,
                        "page": page_num + 1,
                        "table_index": table_index + 1,
                        "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_table_{table_index + 1}",
                        "bbox": convert_bbox_to_mongodb_format(data['bbox'])
                    }
                    # ‚úÖ Original chunk: ‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings (‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
                    page_results['table_chunks'].append(table_chunk)
                    table_chunk_counter += 1
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö processed chunk
                    print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á summary...")
                    summary_text = summarize_with_openai(table_chunk["text"], "table")
                    print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings...")
                    table_processed_chunk = table_chunk.copy()
                    table_processed_chunk["summary"] = summary_text
                    table_processed_chunk["embeddings"] = create_embeddings(summary_text)  # embeddings ‡∏à‡∏≤‡∏Å summary
                    table_processed_chunk["created_at"] = datetime.now()
                    page_results['table_processed_chunks'].append(table_processed_chunk)
                    print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡πÅ‡∏•‡πâ‡∏ß")
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤
        if not page_results['has_content']:
            print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏°‡∏µ text, images, ‡∏´‡∏£‡∏∑‡∏≠ tables)")
        else:
            total_chunks = (len(page_results['text_chunks']) + 
                          len(page_results['image_chunks']) + 
                          len(page_results['table_chunks']))
            print(f"\n‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏™‡∏£‡πá‡∏à: {total_chunks} chunks")
            print(f"   üìù Text: {len(page_results['text_chunks'])} chunks")
            print(f"   üñºÔ∏è Image: {len(page_results['image_chunks'])} chunks")
            print(f"   üìä Table: {len(page_results['table_chunks'])} chunks")
        
        return page_results
        
    except Exception as e:
        print(f"‚ùó Error processing page {page_num + 1}: {e}")
        import traceback
        traceback.print_exc()
        return page_results

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
def store_page_results_to_mongodb(page_results, client, is_first_page=False):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏•‡∏á MongoDB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    
    Args:
        page_results: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å process_single_page()
        client: MongoDB client (‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß)
        is_first_page: ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πà‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
    """
    try:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° databases ‡πÅ‡∏•‡∏∞ collections
        db_original = client[ORIGINAL_DB_NAME]
        db_summary = client[SUMMARY_DB_NAME]
        
        orig_text_col = db_original[ORIGINAL_TEXT_COLLECTION]
        orig_image_col = db_original[ORIGINAL_IMAGE_COLLECTION]
        orig_table_col = db_original[ORIGINAL_TABLE_COLLECTION]
        
        proc_text_col = db_summary[PROCESSED_TEXT_COLLECTION]
        proc_image_col = db_summary[PROCESSED_IMAGE_COLLECTION]
        proc_table_col = db_summary[PROCESSED_TABLE_COLLECTION]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
        if is_first_page:
            print("üóëÔ∏è ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô MongoDB...")
            orig_text_col.delete_many({})
            orig_image_col.delete_many({})
            orig_table_col.delete_many({})
            proc_text_col.delete_many({})
            proc_image_col.delete_many({})
            proc_table_col.delete_many({})
            print("‚úÖ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° created_at ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å chunk
        now = datetime.now()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Original Data
        if page_results['text_chunks']:
            for chunk in page_results['text_chunks']:
                chunk['created_at'] = now
            orig_text_col.insert_many(page_results['text_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['text_chunks'])} text chunks (original)")
        
        if page_results['image_chunks']:
            for chunk in page_results['image_chunks']:
                chunk['created_at'] = now
            orig_image_col.insert_many(page_results['image_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['image_chunks'])} image chunks (original)")
        
        if page_results['table_chunks']:
            for chunk in page_results['table_chunks']:
                chunk['created_at'] = now
            orig_table_col.insert_many(page_results['table_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['table_chunks'])} table chunks (original)")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Processed Data (‡∏°‡∏µ summary ‡πÅ‡∏•‡∏∞ embeddings ‡πÅ‡∏•‡πâ‡∏ß)
        if page_results['text_processed_chunks']:
            for chunk in page_results['text_processed_chunks']:
                if 'created_at' not in chunk:
                    chunk['created_at'] = now
            proc_text_col.insert_many(page_results['text_processed_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['text_processed_chunks'])} text chunks (processed)")
        
        if page_results['image_processed_chunks']:
            for chunk in page_results['image_processed_chunks']:
                if 'created_at' not in chunk:
                    chunk['created_at'] = now
            proc_image_col.insert_many(page_results['image_processed_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['image_processed_chunks'])} image chunks (processed)")
        
        if page_results['table_processed_chunks']:
            for chunk in page_results['table_processed_chunks']:
                if 'created_at' not in chunk:
                    chunk['created_at'] = now
            proc_table_col.insert_many(page_results['table_processed_chunks'])
            print(f"   ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(page_results['table_processed_chunks'])} table chunks (processed)")
        
        return True
        
    except Exception as e:
        print(f"‚ùó Error storing page results to MongoDB: {e}")
        import traceback
        traceback.print_exc()
        return False

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‚Üí loop ‡∏ï‡πà‡∏≠)
def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Pipeline: Extract ‚Üí OCR + PyThaiNLP ‚Üí Summary ‚Üí Embedding ‚Üí Store")
    print("üìÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å MongoDB ‚Üí loop ‡∏ï‡πà‡∏≠")
    print()
    
    client = None
    pymupdf_doc = None
    pdfplumber_pdf = None
    
    try:
        # === INITIALIZATION ===
        print("=== INITIALIZATION ===")
        check_memory()
        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏±‡πâ‡∏á PyMuPDF ‡πÅ‡∏•‡∏∞ pdfplumber
        pymupdf_doc = fitz.open(PDF_PATH)
        pdfplumber_pdf = pdfplumber.open(PDF_PATH)
        ocr_reader = get_ocr_reader()
        
        total_pages = len(pymupdf_doc)
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_pages} ‡∏´‡∏ô‡πâ‡∏≤")
        
        # ‡πÄ‡∏õ‡∏¥‡∏î MongoDB connection ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏•‡∏≠‡∏î‡∏ó‡∏±‡πâ‡∏á pipeline)
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        total_text_chunks = 0
        total_image_chunks = 0
        total_table_chunks = 0
        total_text_processed = 0
        total_image_processed = 0
        total_table_processed = 0
        
        doc_id_counter = 1  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        
        # === LOOP: More Pages (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤) ===
        print("\n=== STEP 1: PAGE-BY-PAGE PROCESSING & STORING ===")
        for page_num in range(total_pages):
            print(f"\n{'='*60}")
            print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}/{total_pages}")
            print(f"{'='*60}")
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Extract ‚Üí Summary ‚Üí Embedding)
            page_results = process_single_page(
                page_num=page_num,
                pymupdf_page=pymupdf_doc[page_num],
                pdfplumber_pdf=pdfplumber_pdf,
                ocr_reader=ocr_reader,
                doc_id_counter=doc_id_counter
            )
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô)
            is_first_page = (page_num == 0)
            print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡∏•‡∏á MongoDB...")
            
            success = store_page_results_to_mongodb(page_results, client, is_first_page=is_first_page)
            
            if success:
                # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks
                total_text_chunks += len(page_results['text_chunks'])
                total_image_chunks += len(page_results['image_chunks'])
                total_table_chunks += len(page_results['table_chunks'])
                total_text_processed += len(page_results['text_processed_chunks'])
                total_image_processed += len(page_results['image_processed_chunks'])
                total_table_processed += len(page_results['table_processed_chunks'])
                
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            else:
                print(f"‚ö†Ô∏è ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠...")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 ‡∏´‡∏ô‡πâ‡∏≤
            if (page_num + 1) % 5 == 0:
                check_memory()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å‡πÑ‡∏´‡∏° (More Pages Decision)
            if page_num < total_pages - 1:
                print(f"‚û°Ô∏è ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å {total_pages - page_num - 1} ‡∏´‡∏ô‡πâ‡∏≤")
            else:
                print(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ({total_pages} ‡∏´‡∏ô‡πâ‡∏≤)")
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF
        pymupdf_doc.close()
        pdfplumber_pdf.close()
        pymupdf_doc = None
        pdfplumber_pdf = None
        
        # === ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ===
        print("\n" + "="*60)
        print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        print("="*60)
        print(f"   üìù Text chunks (original): {total_text_chunks}")
        print(f"   üñºÔ∏è Image chunks (original): {total_image_chunks}")
        print(f"   üìä Table chunks (original): {total_table_chunks}")
        print(f"   üìù Text chunks (processed): {total_text_processed}")
        print(f"   üñºÔ∏è Image chunks (processed): {total_image_processed}")
        print(f"   üìä Table chunks (processed): {total_table_processed}")
        print(f"   üìä Total processed chunks: {total_text_processed + total_image_processed + total_table_processed}")
        
        print("\n‚úÖ Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô MongoDB:")
        print(f"   - Original: {ORIGINAL_DB_NAME}")
        print(f"   - Summary: {SUMMARY_DB_NAME}")
        
    except Exception as e:
        print(f"‚ùó Error in main pipeline: {e}")
        import traceback
        traceback.print_exc()
        print("üîÑ Running garbage collection...")
        gc.collect()
        check_memory()
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if client:
            try:
                db_original = client[ORIGINAL_DB_NAME]
                db_summary = client[SUMMARY_DB_NAME]
                
                orig_text_count = db_original[ORIGINAL_TEXT_COLLECTION].count_documents({})
                proc_text_count = db_summary[PROCESSED_TEXT_COLLECTION].count_documents({})
                
                print(f"\n‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß:")
                print(f"   - Original text chunks: {orig_text_count}")
                print(f"   - Processed text chunks: {proc_text_count}")
            except:
                pass
        
    finally:
        # ‡∏õ‡∏¥‡∏î MongoDB connection
        if client:
            try:
                client.close()
                print("üîå ‡∏õ‡∏¥‡∏î MongoDB connection")
            except:
                pass
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡∏î)
        if pymupdf_doc:
            try:
                pymupdf_doc.close()
            except:
                pass
        if pdfplumber_pdf:
            try:
                pdfplumber_pdf.close()
            except:
                pass

if __name__ == "__main__":
    main()