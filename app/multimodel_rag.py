import os
import io
import base64
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
from dotenv import load_dotenv
from pymongo import MongoClient
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
import torch
import easyocr
from openai import OpenAI
from datetime import datetime
import json
import gc
import psutil
import re

# üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° PyThaiNLP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á OCR
try:
    from pythainlp import word_tokenize
    from pythainlp.spell import correct
    from pythainlp.util import normalize
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ PyThaiNLP loaded successfully")
except ImportError:
    PYTHAINLP_AVAILABLE = False
    print("‚ö†Ô∏è PyThaiNLP not available, using basic text processing")

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ MPS device, PIL.ANTIALIAS ‡πÅ‡∏•‡∏∞ tokenizers parallelism
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î .env
dotenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".env"))
load_dotenv(dotenv_path)

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö
PDF_PATH = "data/attention.pdf"
MONGO_URL = os.getenv("MONGO_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUMMARY_DB_NAME = "astrobot_summary"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà summary ‡πÅ‡∏•‡∏∞ summary embedding ‡πÅ‡∏•‡πâ‡∏ß
ORIGINAL_DB_NAME = "astrobot_original"  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà extract ‡πÅ‡∏•‡πâ‡∏ß

# ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏£‡∏∞‡∏ö‡∏ö - Collection Names
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (ORIGINAL_DB_NAME)
ORIGINAL_TEXT_COLLECTION = "original_text_chunks"
ORIGINAL_IMAGE_COLLECTION = "original_image_chunks"
ORIGINAL_TABLE_COLLECTION = "original_table_chunks"

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (SUMMARY_DB_NAME)
PROCESSED_TEXT_COLLECTION = "processed_text_chunks"
PROCESSED_IMAGE_COLLECTION = "processed_image_chunks"
PROCESSED_TABLE_COLLECTION = "processed_table_chunks"

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory
def check_memory():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory"""
    memory = psutil.virtual_memory()
    print(f"üíæ Memory: {memory.percent}% ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")
    if memory.percent > 80:
        print("‚ö†Ô∏è High memory usage, running garbage collection...")
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

# üÜï ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
def improve_thai_ocr_text(ocr_text):
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å OCR ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
    """
    if not PYTHAINLP_AVAILABLE or not ocr_text.strip():
        return ocr_text
    
    try:
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text = ocr_text.strip()
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î
        text = re.sub(r'([‡∏Å-‡πô])([A-Za-z])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
        text = re.sub(r'([A-Za-z])([‡∏Å-‡πô])', r'\1 \2', text)  # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©-‡πÑ‡∏ó‡∏¢
        text = re.sub(r'([‡∏Å-‡πô])([0-9])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ó‡∏¢-‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        text = re.sub(r'([0-9])([‡∏Å-‡πô])', r'\1 \2', text)    # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç-‡πÑ‡∏ó‡∏¢
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
        text = re.sub(r'\s+', ' ', text)
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
        words = word_tokenize(text, engine='newmm')
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
        corrected_words = []
        for word in words:
            if len(word) > 2 and word.isalpha():  # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                try:
                    corrected = correct(word)
                    corrected_words.append(corrected if corrected else word)
                except:
                    corrected_words.append(word)
            else:
                corrected_words.append(word)
        
        # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        improved_text = ' '.join(corrected_words)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        improved_text = re.sub(r'\s+', ' ', improved_text).strip()
        
        return improved_text
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error in Thai text improvement: {e}")
        return ocr_text

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö lazy loading
def get_embedding_model():
    """‡πÇ‡∏´‡∏•‡∏î embedding model ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_embedding_model, 'model'):
        print("üîÑ Loading embedding model...")
        get_embedding_model.model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
    return get_embedding_model.model

def get_semantic_model():
    """‡πÇ‡∏´‡∏•‡∏î semantic model ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_semantic_model, 'model'):
        print("üîÑ Loading semantic model...")
        get_semantic_model.model = SentenceTransformer("minishlab/potion-multilingual-128M", device="cpu")
    return get_semantic_model.model

def get_ocr_reader():
    """‡πÇ‡∏´‡∏•‡∏î OCR reader ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_ocr_reader, 'reader'):
        print(" Loading OCR reader...")
        get_ocr_reader.reader = easyocr.Reader(['en', 'th'], gpu=False, verbose=False)
    return get_ocr_reader.reader

# OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
def extract_text_with_pymupdf(path):
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏î‡πâ‡∏ß‡∏¢ PyMuPDF
    """
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    text_output = ""
    doc = fitz.open(path)
    
    try:
        for page_num, page in enumerate(doc):
            page_text = page.get_text("text")
            if page_text.strip():
                text_output += f"\n--- ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ---\n{page_text}"
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 20 ‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 20 == 0:
                check_memory()
                
    finally:
        doc.close()
    
    return text_output

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory management)
def extract_images_with_ocr(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OCR + PyThaiNLP
    """
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    images_data = []
    doc = fitz.open(path)
    
    try:
        ocr_reader = get_ocr_reader()
        
        for page_num, page in enumerate(doc):
            images = page.get_images(full=True)
            print(f"‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}: {len(images)} ‡∏£‡∏π‡∏õ")
            
            for img_index, img in enumerate(images):
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    image = Image.open(io.BytesIO(image_bytes))
                    width, height = image.size
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width * height > 1500000:  # 1.5M pixels
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà {img_index + 1} ({width}x{height})")
                        continue
                    
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if width < 50 or height < 50:
                        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å {img_index + 1} ({width}x{height})")
                        continue
                    
                    # OCR
                    ocr_results = ocr_reader.readtext(image_bytes)
                    ocr_text = " ".join([result[1] for result in ocr_results if result[2] > 0.3])  # ‡∏•‡∏î confidence threshold
                    
                    if ocr_text.strip():
                        # üÜï ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                        improved_text = improve_thai_ocr_text(ocr_text)
                        
                        image_info = {
                            "page": page_num + 1,
                            "image_index": img_index + 1,
                            "original_text": ocr_text.strip(),
                            "improved_text": improved_text,
                            "text": improved_text,  # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                            "image_base64": base64.b64encode(image_bytes).decode("utf-8")
                        }
                        images_data.append(image_info)
                        
                        print(f"‚úÖ ‡∏£‡∏π‡∏õ {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                    
                    # ‡∏•‡πâ‡∏≤‡∏á memory
                    del image, image_bytes, ocr_results
                    
                except Exception as e:
                    print(f"‚ùó Error processing image {img_index + 1} on page {page_num + 1}: {e}")
                    continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
            if page_num % 5 == 0:
                check_memory()
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
            if len(images_data) > 50:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 50 ‡∏£‡∏π‡∏õ
                print("‚ö†Ô∏è ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 50 ‡∏£‡∏π‡∏õ")
                break
                
    finally:
        doc.close()
    
    return images_data

# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber
def extract_tables_with_pdfplumber(path):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô PDF ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ pdfplumber
    """
    print(f" ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å: {path}")
    tables_data = []
    
    try:
        with pdfplumber.open(path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                for table_index, table in enumerate(tables):
                    if table:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                        table_text = ""
                        for row in table:
                            if row:
                                row_text = " | ".join([cell if cell else "" for cell in row])
                                table_text += row_text + "\n"
                        
                        if table_text.strip():
                            table_info = {
                                "page": page_num + 1,
                                "table_index": table_index + 1,
                                "text": table_text.strip()
                            }
                            tables_data.append(table_info)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 10 ‡∏´‡∏ô‡πâ‡∏≤
                if page_num % 10 == 0:
                    check_memory()
                    
    except Exception as e:
        print(f"‚ùó Error extracting tables: {e}")
    
    return tables_data

# ‚úÖ Semantic Chunking ‡∏î‡πâ‡∏ß‡∏¢ Potion Model
def semantic_chunking_with_potion(text, content_type, chunk_size=1000, overlap=200):
    """
    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ Semantic Chunking ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Potion Model
    """
    print(f"üß† ‡πÄ‡∏£‡∏¥‡πà‡∏° Semantic Chunking ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {content_type.upper()}")
    
    try:
        semantic_model = get_semantic_model()
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        sentences = text.split('. ')
        if len(sentences) <= 1:
            return [{"text": text, "type": content_type, "chunk_id": 0}]
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
        if len(sentences) > 500:
            sentences = sentences[:500]
            print(f"‚ö†Ô∏è ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà 500 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        sentence_embeddings = semantic_model.encode(sentences)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        chunks = []
        current_chunk = []
        current_length = 0
        
        for i, sentence in enumerate(sentences):
            current_chunk.append(sentence)
            current_length += len(sentence)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ö‡πà‡∏á chunk ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if current_length >= chunk_size or i == len(sentences) - 1:
                chunk_text = '. '.join(current_chunk)
                chunks.append({
                    "text": chunk_text,
                    "type": content_type,
                    "chunk_id": len(chunks)
                })
                current_chunk = []
                current_length = 0
        
        # ‡∏•‡πâ‡∏≤‡∏á memory
        del sentence_embeddings, sentences
        check_memory()
        
        return chunks
        
    except Exception as e:
        print(f"‚ùó Error in semantic chunking: {e}")
        # Fallback: ‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
        return [{"text": text, "type": content_type, "chunk_id": 0}]

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Embeddings
def create_embeddings(text):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    """
    try:
        embedding_model = get_embedding_model()
        return embedding_model.encode(text).tolist()
    except Exception as e:
        print(f"‚ùó Error creating embeddings: {e}")
        return [0.0] * 384  # fallback vector

# ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OpenAI
def summarize_with_openai(text, content_type):
    """
    ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ OpenAI GPT
    """
    try:
        prompt = f"""
        ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢):
        
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {content_type}
        ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {text[:2000]}...
        
        ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        """
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=150,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"‚ùó Error in summarization: {e}")
        return text[:200] + "..." if len(text) > 200 else text

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á MongoDB (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
def store_original_data_in_mongodb(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á ORIGINAL_DB_NAME (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡πÉ‡∏ä‡πâ ORIGINAL_DB_NAME ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        db_name = ORIGINAL_DB_NAME
        print(f"üìä ‡πÉ‡∏ä‡πâ Database: {db_name} (Original - ‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings/summary)")
        
        db = client[db_name]
        collection = db[collection_name]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        collection.delete_many({})
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now()
            
            # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° embeddings ‡πÅ‡∏•‡∏∞ summary
            collection.insert_one(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(chunks)} chunks ‡∏•‡∏á {collection_name}")
        client.close()
        
    except Exception as e:
        print(f"‚ùó MongoDB Atlas connection failed: {e}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏ó‡∏ô...")
        
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        store_original_to_json(chunks, collection_name)

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á MongoDB (‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
def store_processed_data_in_mongodb(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á SUMMARY_DB_NAME (‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas
        print(f"üîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas...")
        client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        client.admin.command('ping')
        print(f"‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MongoDB Atlas ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        # ‡πÉ‡∏ä‡πâ SUMMARY_DB_NAME ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
        db_name = SUMMARY_DB_NAME
        print(f"üìä ‡πÉ‡∏ä‡πâ Database: {db_name} (Processed - ‡∏°‡∏µ summary embeddings/summary)")
        
        db = client[db_name]
        collection = db[collection_name]
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        collection.delete_many({})
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (‡∏°‡∏µ summary embeddings ‡πÅ‡∏•‡∏∞ summary)
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
            processed_chunk = chunk.copy()
            processed_chunk["created_at"] = datetime.now()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏Å‡πà‡∏≠‡∏ô
            summary_text = summarize_with_openai(chunk["text"], chunk["type"])
            processed_chunk["summary"] = summary_text
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏à‡∏≤‡∏Å summary ‡πÅ‡∏ó‡∏ô text ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            processed_chunk["embeddings"] = create_embeddings(summary_text)
            
            collection.insert_one(processed_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 3 chunks
            if i % 3 == 0:
                check_memory()
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß {len(chunks)} chunks ‡∏•‡∏á {collection_name}")
        client.close()
        
    except Exception as e:
        print(f"‚ùó MongoDB Atlas connection failed: {e}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏ó‡∏ô...")
        
        # Fallback: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON
        store_processed_to_json(chunks, collection_name)

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (fallback)
def store_original_to_json(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô fallback (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
        original_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° created_at
            original_chunk = chunk.copy()
            original_chunk["created_at"] = datetime.now().isoformat()
            
            # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° embeddings ‡πÅ‡∏•‡∏∞ summary
            original_chunks.append(original_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 chunks
            if i % 5 == 0:
                check_memory()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        filename = f"{output_dir}/{collection_name}_original.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(original_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö {len(original_chunks)} chunks ‡∏•‡∏á {filename}")
        
    except Exception as e:
        print(f"‚ùó Error saving original data to JSON: {e}")

# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON (fallback)
def store_processed_to_json(chunks, collection_name):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏õ‡πá‡∏ô fallback (‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        output_dir = "output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunks
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"üìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• chunk {i+1}/{len(chunks)}...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á chunk ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
            processed_chunk = chunk.copy()
            processed_chunk["created_at"] = datetime.now().isoformat()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏Å‡πà‡∏≠‡∏ô
            summary_text = summarize_with_openai(chunk["text"], chunk["type"])
            processed_chunk["summary"] = summary_text
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏à‡∏≤‡∏Å summary ‡πÅ‡∏ó‡∏ô text ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            processed_chunk["embeddings"] = create_embeddings(summary_text)
            processed_chunks.append(processed_chunk)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 3 chunks
            if i % 3 == 0:
                check_memory()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        filename = f"{output_dir}/{collection_name}_processed.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(processed_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß {len(processed_chunks)} chunks ‡∏•‡∏á {filename}")
        
    except Exception as e:
        print(f"‚ùó Error saving processed data to JSON: {e}")

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏ï‡∏≤‡∏° flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö)
def process_single_page(page_num, pymupdf_page, pdfplumber_pdf, ocr_reader, doc_id_counter):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: Extract ‚Üí Summary ‚Üí Embedding ‚Üí Store
    ‡∏ï‡∏≤‡∏° flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö: ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥ summary/embedding ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    
    Args:
        page_num: ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (0-based)
        pymupdf_page: ‡∏´‡∏ô‡πâ‡∏≤ PDF ‡∏à‡∏≤‡∏Å PyMuPDF
        pdfplumber_pdf: PDF object ‡∏à‡∏≤‡∏Å pdfplumber
        ocr_reader: OCR reader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        doc_id_counter: counter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        
    Returns:
        dict: {
            'has_content': bool,  # ‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤)
            'text_chunks': list,
            'image_chunks': list,
            'table_chunks': list,
            'text_processed_chunks': list,
            'image_processed_chunks': list,
            'table_processed_chunks': list
        }
    """
    page_results = {
        'has_content': False,
        'text_chunks': [],
        'image_chunks': [],
        'table_chunks': [],
        'text_processed_chunks': [],
        'image_processed_chunks': [],
        'table_processed_chunks': []
    }
    
    try:
        print(f"\n{'='*50}")
        print(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}")
        print(f"{'='*50}")
        
        # === EXTRACT: Text, Images, Tables ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ===
        # 1. Extract Text (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô)
        page_text = pymupdf_page.get_text("text")
        if page_text.strip():
            page_results['has_content'] = True
            print(f"‚úÖ ‡∏û‡∏ö Text: {len(page_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
            
            # Create text chunk
            text_chunk = {
                "text": page_text.strip(),
                "type": "text",
                "chunk_id": len(page_results['text_chunks']),
                "page": page_num + 1,
                "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_text"
            }
            page_results['text_chunks'].append(text_chunk)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            summary_text = summarize_with_openai(text_chunk["text"], "text")
            text_processed_chunk = text_chunk.copy()
            text_processed_chunk["summary"] = summary_text
            text_processed_chunk["embeddings"] = create_embeddings(summary_text)
            text_processed_chunk["created_at"] = datetime.now()
            page_results['text_processed_chunks'].append(text_processed_chunk)
            print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡πÅ‡∏•‡πâ‡∏ß")
        
        # 2. Extract Images (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô)
        images = pymupdf_page.get_images(full=True)
        print(f"üîç ‡∏û‡∏ö {len(images)} ‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ")
        
        for img_index, img in enumerate(images):
            try:
                xref = img[0]
                base_image = pymupdf_page.parent.extract_image(xref)
                image_bytes = base_image["image"]
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                image = Image.open(io.BytesIO(image_bytes))
                width, height = image.size
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                if width * height > 1500000:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà {img_index + 1} ({width}x{height})")
                    continue
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                if width < 50 or height < 50:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡πá‡∏Å {img_index + 1} ({width}x{height})")
                    continue
                
                # OCR
                ocr_results = ocr_reader.readtext(image_bytes)
                ocr_text = " ".join([result[1] for result in ocr_results if result[2] > 0.3])
                
                if ocr_text.strip():
                    page_results['has_content'] = True
                    # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
                    improved_text = improve_thai_ocr_text(ocr_text)
                    
                    print(f"‚úÖ ‡∏£‡∏π‡∏õ {img_index + 1}: {len(improved_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                    
                    # Create image chunk
                    image_chunk = {
                        "text": improved_text,
                        "type": "image",
                        "chunk_id": len(page_results['image_chunks']),
                        "page": page_num + 1,
                        "image_index": img_index + 1,
                        "original_text": ocr_text.strip(),
                        "improved_text": improved_text,
                        "image_base64": base64.b64encode(image_bytes).decode("utf-8"),
                        "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_img_{img_index + 1}"
                    }
                    page_results['image_chunks'].append(image_chunk)
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                    summary_text = summarize_with_openai(image_chunk["text"], "image")
                    image_processed_chunk = image_chunk.copy()
                    image_processed_chunk["summary"] = summary_text
                    image_processed_chunk["embeddings"] = create_embeddings(summary_text)
                    image_processed_chunk["created_at"] = datetime.now()
                    page_results['image_processed_chunks'].append(image_processed_chunk)
                    print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡πÅ‡∏•‡πâ‡∏ß")
                
                # ‡∏•‡πâ‡∏≤‡∏á memory
                del image, image_bytes, ocr_results
                
            except Exception as e:
                print(f"‚ùó Error processing image {img_index + 1} on page {page_num + 1}: {e}")
                continue
        
        # 3. Extract Tables (‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô)
        if page_num < len(pdfplumber_pdf.pages):
            pdfplumber_page = pdfplumber_pdf.pages[page_num]
            tables = pdfplumber_page.extract_tables()
            print(f"üîç ‡∏û‡∏ö {len(tables)} ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ")
            
            for table_index, table in enumerate(tables):
                if table:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                    table_text = ""
                    for row in table:
                        if row:
                            row_text = " | ".join([cell if cell else "" for cell in row])
                            table_text += row_text + "\n"
                    
                    if table_text.strip():
                        page_results['has_content'] = True
                        print(f"‚úÖ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á {table_index + 1}: {len(table_text)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
                        
                        # Create table chunk
                        table_chunk = {
                            "text": table_text.strip(),
                            "type": "table",
                            "chunk_id": len(page_results['table_chunks']),
                            "page": page_num + 1,
                            "table_index": table_index + 1,
                            "doc_id": f"doc_{doc_id_counter}_{page_num + 1}_table_{table_index + 1}"
                        }
                        page_results['table_chunks'].append(table_chunk)
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                        summary_text = summarize_with_openai(table_chunk["text"], "table")
                        table_processed_chunk = table_chunk.copy()
                        table_processed_chunk["summary"] = summary_text
                        table_processed_chunk["embeddings"] = create_embeddings(summary_text)
                        table_processed_chunk["created_at"] = datetime.now()
                        page_results['table_processed_chunks'].append(table_processed_chunk)
                        print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ embeddings ‡πÅ‡∏•‡πâ‡∏ß")
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤
        if not page_results['has_content']:
            print(f"‚ö†Ô∏è ‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏°‡∏µ text, images, ‡∏´‡∏£‡∏∑‡∏≠ tables)")
        else:
            total_chunks = (len(page_results['text_chunks']) + 
                          len(page_results['image_chunks']) + 
                          len(page_results['table_chunks']))
            print(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} ‡πÄ‡∏™‡∏£‡πá‡∏à: {total_chunks} chunks")
        
        return page_results
        
    except Exception as e:
        print(f"‚ùó Error processing page {page_num + 1}: {e}")
        return page_results

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö)
def main():
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Pipeline: Extract ‚Üí OCR + PyThaiNLP ‚Üí Summary ‚Üí Embedding ‚Üí Store")
    print("üìÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥ Summary/Embedding ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏ï‡∏≤‡∏° Flow)")
    print()
    
    try:
        # === INITIALIZATION ===
        print("=== INITIALIZATION ===")
        check_memory()
        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏±‡πâ‡∏á PyMuPDF ‡πÅ‡∏•‡∏∞ pdfplumber
        pymupdf_doc = fitz.open(PDF_PATH)
        pdfplumber_pdf = pdfplumber.open(PDF_PATH)
        ocr_reader = get_ocr_reader()
        
        total_pages = len(pymupdf_doc)
        print(f"üìö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_pages} ‡∏´‡∏ô‡πâ‡∏≤")
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        all_text_chunks = []
        all_image_chunks = []
        all_table_chunks = []
        all_text_processed_chunks = []
        all_image_processed_chunks = []
        all_table_processed_chunks = []
        
        doc_id_counter = 1  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á doc_id
        
        # === LOOP: More Pages (‡∏ï‡∏≤‡∏° flow ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö) ===
        print("\n=== STEP 1: PAGE-BY-PAGE PROCESSING ===")
        for page_num in range(total_pages):
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Extract ‚Üí Summary ‚Üí Embedding)
            page_results = process_single_page(
                page_num=page_num,
                pymupdf_page=pymupdf_doc[page_num],
                pdfplumber_pdf=pdfplumber_pdf,
                ocr_reader=ocr_reader,
                doc_id_counter=doc_id_counter
            )
            
            # ‡∏£‡∏ß‡∏° chunks ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            all_text_chunks.extend(page_results['text_chunks'])
            all_image_chunks.extend(page_results['image_chunks'])
            all_table_chunks.extend(page_results['table_chunks'])
            all_text_processed_chunks.extend(page_results['text_processed_chunks'])
            all_image_processed_chunks.extend(page_results['image_processed_chunks'])
            all_table_processed_chunks.extend(page_results['table_processed_chunks'])
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡∏ó‡∏∏‡∏Å 5 ‡∏´‡∏ô‡πâ‡∏≤
            if (page_num + 1) % 5 == 0:
                check_memory()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å‡πÑ‡∏´‡∏° (More Pages Decision)
            if page_num < total_pages - 1:
                print(f"‚û°Ô∏è ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å {total_pages - page_num - 1} ‡∏´‡∏ô‡πâ‡∏≤")
            else:
                print(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ({total_pages} ‡∏´‡∏ô‡πâ‡∏≤)")
                print("‚û°Ô∏è ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà SentenceTransformer ‡πÅ‡∏•‡∏∞ MongoDB collection summary")
        
        # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF
        pymupdf_doc.close()
        pdfplumber_pdf.close()
        
        # === STEP 2: STORE IN MONGODB ===
        print("\n=== STEP 2: STORE IN MONGODB ===")
        check_memory()
        
        print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:")
        print(f"   üìù Text chunks: {len(all_text_chunks)}")
        print(f"   üñºÔ∏è Image chunks: {len(all_image_chunks)}")
        print(f"   üìä Table chunks: {len(all_table_chunks)}")
        print(f"   üìä Processed chunks: {len(all_text_processed_chunks) + len(all_image_processed_chunks) + len(all_table_processed_chunks)}")
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÉ‡∏ô ORIGINAL_DB_NAME (‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings ‡πÅ‡∏•‡∏∞ summary)
        print("\nüìÅ ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÉ‡∏ô ORIGINAL_DB_NAME...")
        if all_text_chunks:
            store_original_data_in_mongodb(all_text_chunks, ORIGINAL_TEXT_COLLECTION)
        if all_image_chunks:
            store_original_data_in_mongodb(all_image_chunks, ORIGINAL_IMAGE_COLLECTION)
        if all_table_chunks:
            store_original_data_in_mongodb(all_table_chunks, ORIGINAL_TABLE_COLLECTION)
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (‡∏°‡∏µ summary embedding ‡πÅ‡∏•‡∏∞ summary) ‡πÉ‡∏ô SUMMARY_DB_NAME
        print("\nüìä ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô SUMMARY_DB_NAME...")
        print("   (Summary embeddings ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å summary text ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ SentenceTransformer)")
        if all_text_processed_chunks:
            # ‡πÉ‡∏ä‡πâ store_processed_data_in_mongodb ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á summary/embeddings 
            # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô process_single_page
            try:
                client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
                client.admin.command('ping')
                db = client[SUMMARY_DB_NAME]
                collection = db[PROCESSED_TEXT_COLLECTION]
                collection.delete_many({})
                collection.insert_many(all_text_processed_chunks)
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(all_text_processed_chunks)} processed text chunks")
                client.close()
            except Exception as e:
                print(f"‚ùó Error storing processed text chunks: {e}")
        
        if all_image_processed_chunks:
            try:
                client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
                client.admin.command('ping')
                db = client[SUMMARY_DB_NAME]
                collection = db[PROCESSED_IMAGE_COLLECTION]
                collection.delete_many({})
                collection.insert_many(all_image_processed_chunks)
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(all_image_processed_chunks)} processed image chunks")
                client.close()
            except Exception as e:
                print(f"‚ùó Error storing processed image chunks: {e}")
        
        if all_table_processed_chunks:
            try:
                client = MongoClient(MONGO_URL, serverSelectionTimeoutMS=5000)
                client.admin.command('ping')
                db = client[SUMMARY_DB_NAME]
                collection = db[PROCESSED_TABLE_COLLECTION]
                collection.delete_many({})
                collection.insert_many(all_table_processed_chunks)
                print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(all_table_processed_chunks)} processed table chunks")
                client.close()
            except Exception as e:
                print(f"‚ùó Error storing processed table chunks: {e}")
        
        print("\n‚úÖ Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô MongoDB:")
        print(f"   - Original: {ORIGINAL_DB_NAME}")
        print(f"   - Summary: {SUMMARY_DB_NAME}")
        
    except Exception as e:
        print(f"‚ùó Error in main pipeline: {e}")
        import traceback
        traceback.print_exc()
        print("üîÑ Running garbage collection...")
        gc.collect()
        check_memory()

if __name__ == "__main__":
    main()