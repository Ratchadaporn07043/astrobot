# üñºÔ∏è ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ Embed ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå

## üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ:
1. **OCR + Text Embedding**
   - ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ EasyOCR
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡∏î‡πâ‡∏ß‡∏¢ OpenAI
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á text embeddings ‡∏à‡∏≤‡∏Å summary

### ‚ùå ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ:
1. **Image Embedding** - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Vision Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏≤‡∏á‡∏™‡∏≤‡∏¢‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
2. **Structured Data Extraction** - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥ (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏î‡∏≤‡∏ß, ‡∏°‡∏∏‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå)

---

## üöÄ ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç

### 1. ‡πÄ‡∏û‡∏¥‡πà‡∏° Image Embedding ‡∏î‡πâ‡∏ß‡∏¢ Vision Model

#### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ OpenAI Vision API (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
```python
from openai import OpenAI

def create_image_embedding_with_openai(image_bytes):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding ‡∏î‡πâ‡∏ß‡∏¢ OpenAI Vision API
    """
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # ‡πÅ‡∏õ‡∏•‡∏á image bytes ‡πÄ‡∏õ‡πá‡∏ô base64
    import base64
    image_base64 = base64.b64encode(image_bytes).decode('utf-8')
    
    # ‡πÉ‡∏ä‡πâ Vision API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
    response = client.embeddings.create(
        model="text-embedding-3-large",  # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ vision model
        input=image_base64
    )
    
    return response.data[0].embedding
```

#### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ CLIP Model (Open Source)
```python
import torch
from transformers import CLIPProcessor, CLIPModel

def get_clip_model():
    """‡πÇ‡∏´‡∏•‡∏î CLIP model ‡πÅ‡∏ö‡∏ö lazy loading"""
    if not hasattr(get_clip_model, 'model'):
        print("üîÑ Loading CLIP model...")
        get_clip_model.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        get_clip_model.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    return get_clip_model.model, get_clip_model.processor

def create_image_embedding_with_clip(image_bytes):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding ‡∏î‡πâ‡∏ß‡∏¢ CLIP
    """
    model, processor = get_clip_model()
    
    # ‡πÅ‡∏õ‡∏•‡∏á image bytes ‡πÄ‡∏õ‡πá‡∏ô PIL Image
    from PIL import Image
    import io
    image = Image.open(io.BytesIO(image_bytes))
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ CLIP
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
    
    return image_features[0].tolist()
```

#### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 3: ‡πÉ‡∏ä‡πâ Sentence Transformers (Multimodal)
```python
from sentence_transformers import SentenceTransformer

def get_multimodal_model():
    """‡πÇ‡∏´‡∏•‡∏î multimodal model"""
    if not hasattr(get_multimodal_model, 'model'):
        print("üîÑ Loading multimodal model...")
        # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á text ‡πÅ‡∏•‡∏∞ image
        get_multimodal_model.model = SentenceTransformer('clip-ViT-B-32')
    return get_multimodal_model.model

def create_image_embedding_with_sentence_transformers(image_bytes):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding ‡∏î‡πâ‡∏ß‡∏¢ Sentence Transformers
    """
    model = get_multimodal_model()
    
    # ‡πÅ‡∏õ‡∏•‡∏á image bytes ‡πÄ‡∏õ‡πá‡∏ô PIL Image
    from PIL import Image
    import io
    image = Image.open(io.BytesIO(image_bytes))
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
    embedding = model.encode(image)
    return embedding.tolist()
```

---

### 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° Structured Data Extraction

#### ‡πÉ‡∏ä‡πâ GPT-4 Vision ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
```python
def extract_astrological_structure_with_gpt4v(image_bytes):
    """
    ‡πÉ‡∏ä‡πâ GPT-4 Vision ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå
    """
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    import base64
    image_base64 = base64.b64encode(image_bytes).decode('utf-8')
    
    prompt = """
    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ô‡∏µ‡πâ:
    
    1. ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏î‡∏≤‡∏ß‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏î‡∏ß‡∏á (‡∏£‡∏≤‡∏®‡∏µ, ‡∏≠‡∏á‡∏®‡∏≤, ‡∏•‡∏¥‡∏õ‡∏î‡∏≤)
    2. ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á House Cusps (‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏£‡∏∑‡∏≠‡∏ô‡∏ä‡∏∞‡∏ï‡∏≤)
    3. ‡∏°‡∏∏‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Aspects) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏î‡∏≤‡∏ß‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    
    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON format:
    {
        "planets": [
            {"name": "Sun", "sign": "Aquarius", "degree": 18, "minute": 30, "house": 1},
            ...
        ],
        "houses": [
            {"number": 1, "cusp_sign": "Aquarius", "cusp_degree": 18},
            ...
        ],
        "aspects": [
            {"planet1": "Sun", "planet2": "Moon", "aspect": "Conjunction", "orb": 5.2},
            ...
        ]
    }
    """
    
    response = client.chat.completions.create(
        model="gpt-4o",  # ‡∏´‡∏£‡∏∑‡∏≠ gpt-4-vision-preview
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}"
                        }
                    }
                ]
            }
        ],
        max_tokens=2000
    )
    
    # Parse JSON response
    import json
    structure_data = json.loads(response.choices[0].message.content)
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö embedding
    structure_text = convert_structure_to_text(structure_data)
    
    return structure_data, structure_text

def convert_structure_to_text(structure_data):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö embedding
    """
    text_parts = []
    
    # Planets
    for planet in structure_data.get("planets", []):
        text_parts.append(
            f"{planet['name']} ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏®‡∏µ{planet['sign']} "
            f"‡∏≠‡∏á‡∏®‡∏≤ {planet['degree']}¬∞{planet.get('minute', 0)}' "
            f"‡πÄ‡∏£‡∏∑‡∏≠‡∏ô‡∏ä‡∏∞‡∏ï‡∏≤‡∏ó‡∏µ‡πà {planet['house']}"
        )
    
    # Aspects
    for aspect in structure_data.get("aspects", []):
        text_parts.append(
            f"{aspect['planet1']} ‡∏ó‡∏≥‡∏°‡∏∏‡∏° {aspect['aspect']} "
            f"‡∏Å‡∏±‡∏ö {aspect['planet2']} (orb: {aspect['orb']}¬∞)"
        )
    
    return ". ".join(text_parts)
```

---

### 3. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô `multimodel_rag.py`

#### ‡πÄ‡∏û‡∏¥‡πà‡∏° Image Embedding ‡πÉ‡∏ô process_single_page
```python
# ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• image
elif element_type == 'image':
    # ... OCR code ...
    
    if ocr_text.strip():
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á image chunk
        image_chunk = {
            "text": improved_text,
            "type": "image",
            # ... existing fields ...
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding
        print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding...")
        image_embedding = create_image_embedding_with_clip(image_bytes)
        image_chunk["image_embedding"] = image_embedding
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå)
        if is_astrological_chart(image_bytes):  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            print(f"   üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á...")
            structure_data, structure_text = extract_astrological_structure_with_gpt4v(image_bytes)
            image_chunk["structure_data"] = structure_data
            image_chunk["structure_text"] = structure_text
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å structure text
            structure_embedding = create_embeddings(structure_text)
            image_chunk["structure_embedding"] = structure_embedding
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary ‡πÅ‡∏•‡∏∞ text embedding (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        summary_text = summarize_with_openai(image_chunk["text"], "image")
        text_embedding = create_embeddings(summary_text)
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á text embedding ‡πÅ‡∏•‡∏∞ image embedding
        image_processed_chunk = image_chunk.copy()
        image_processed_chunk["summary"] = summary_text
        image_processed_chunk["text_embeddings"] = text_embedding
        image_processed_chunk["image_embeddings"] = image_embedding
```

---

## üìã ‡∏™‡∏£‡∏∏‡∏õ

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥‡πÇ‡∏´‡∏£‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:

1. **‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà OCR ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ** ‚Üí ‚úÖ Embed ‡πÑ‡∏î‡πâ (Text Embedding)
2. **‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏≤‡∏á‡∏™‡∏≤‡∏¢‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û** ‚Üí ‚ö†Ô∏è ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° Image Embedding
3. **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á** ‚Üí ‚ö†Ô∏è ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° Structured Data Extraction

### ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
- **‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô**: ‡πÉ‡∏ä‡πâ OCR + Text Embedding (‡∏ó‡∏≥‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß) ‚Üí ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥
- **‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏•‡∏≤‡∏á**: ‡πÄ‡∏û‡∏¥‡πà‡∏° Image Embedding ‡∏î‡πâ‡∏ß‡∏¢ CLIP ‚Üí ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏≤‡∏á‡∏™‡∏≤‡∏¢‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥
- **‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß**: ‡πÄ‡∏û‡∏¥‡πà‡∏° Structured Data Extraction ‡∏î‡πâ‡∏ß‡∏¢ GPT-4 Vision ‚Üí ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á

---

## üîß Implementation Steps

1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies:
```bash
pip install transformers torch sentence-transformers
# ‡∏´‡∏£‡∏∑‡∏≠
pip install openai  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vision API
```

2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á image embedding
3. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç `process_single_page()` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ image embedding
4. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï MongoDB schema ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö image_embeddings
5. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï retrieval system ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á text ‡πÅ‡∏•‡∏∞ image embeddings

